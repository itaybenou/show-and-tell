# Show and Tell: Visually Explainable Deep Neural Nets via Spatially-Aware Concept Bottleneck Models [CVPR'25]

<div>
  <img src="docs/opening_figure_v5.png" style="width:100%">
</div>
<br>

<a href="https://arxiv.org/abs/2502.20134"><img src="https://img.shields.io/badge/arXiv-2502.20134-b31b1b.svg" height=20.5></a>

> **Show and Tell: Visually Explainable Deep Neural Nets via Spatially-Aware Concept Bottleneck Models**
>
> Itay Benou, Tammy Riklin-Raviv
>
> Abstract: Modern deep neural networks have now reached human-level performance across a variety of tasks. However, unlike humans they lack the ability to explain their decisions by showing where and telling what concepts guided them. In this work, we present a unified framework for transforming any vision neural network into a spatially and conceptually interpretable model. We introduce a spatially-aware concept bottleneck layer that projects “black-box” features of pre-trained backbone models into interpretable concept maps, without requiring human labels. By training a classification layer over this bottleneck, we obtain a self-explaining model that articulates which concepts most influenced its prediction, along with heatmaps that ground them in the input image. Accordingly, we name this method “Spatially-Aware and Label-Free Concept Bottleneck Model” (SALF-CBM). Our results show that the proposed SALF-CBM: (1) Outperforms non-spatial CBM methods, as well as the original backbone, on a variety of classification tasks; (2) Produces high-quality spatial explanations, outperforming widely used heatmap-based methods on a zero-shot segmentation task; (3) Facilitates model exploration and debugging, enabling users to query specific image regions and refine the model's decisions by locally editing its concept maps.

# Code
Coming soon!
